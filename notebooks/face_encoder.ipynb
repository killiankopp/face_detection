{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-07T23:36:47.964062Z",
     "start_time": "2024-12-07T23:36:47.614714Z"
    }
   },
   "source": [
    "from src.face_encoder import FaceEncoder, store_encodings\n",
    "\n",
    "face_encoder = FaceEncoder()\n",
    "image_path = '../artefacts/incoming/kiki2.jpg'\n",
    "\n",
    "try:\n",
    "    face_encodings = face_encoder.process_image(image_path)\n",
    "    json_path = store_encodings(image_path, face_encodings)\n",
    "finally:\n",
    "    face_encoder.close()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:335] Error parsing text-format mediapipe.CalculatorGraphConfig: 15:22: Expected identifier, got: \\\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to parse: node {\n  name: \"facedetectionshortrange__facedetection__ToImageCalculator\"\n  calculator: \"ToImageCalculator\"\n  input_stream: \"IMAGE:image\"\n  output_stream: \"IMAGE:facedetectionshortrange__facedetection__multi_backend_image\"\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__ImageToTensorCalculator\"\n  calculator: \"ImageToTensorCalculator\"\n  input_stream: \"IMAGE:facedetectionshortrange__facedetection__multi_backend_image\"\n  output_stream: \"TENSORS:facedetectionshortrange__facedetection__input_tensors\"\n  output_stream: \"MATRIX:facedetectionshortrange__facedetection__transform_matrix\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.ImageToTensorCalculatorOptions\"\n    value: \"\\030\\001\"\\n\\r\\000\\000\\200\\277\\025\\000\\000\\200?0\\001\\010\\200\\001\\020\\200\\001\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__SsdAnchorsCalculator\"\n  calculator: \"SsdAnchorsCalculator\"\n  output_side_packet: \"facedetectionshortrange__facedetection__anchors\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.SsdAnchorsCalculatorOptions\"\n    value: \"\\035\\000\\000\\030>%\\000\\000@?-\\000\\000\\000?5\\000\\000\\000?]\\000\\000\\200?p\\001\\010\\200\\001\\020\\200\\0018\\004P\\010P\\020P\\020P\\020m\\000\\000\\200?\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__inferencecalculator__facedetectionshortrange__facedetection__InferenceCalculator\"\n  calculator: \"InferenceCalculatorCpu\"\n  input_stream: \"TENSORS:facedetectionshortrange__facedetection__input_tensors\"\n  output_stream: \"TENSORS:facedetectionshortrange__facedetection__detection_tensors\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.InferenceCalculatorOptions\"\n    value: \"*\\002\"\\000\\nBmediapipe/modules/face_detection/face_detection_short_range.tflite\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__TensorsToDetectionsCalculator\"\n  calculator: \"TensorsToDetectionsCalculator\"\n  input_stream: \"TENSORS:facedetectionshortrange__facedetection__detection_tensors\"\n  output_stream: \"DETECTIONS:facedetectionshortrange__facedetection__unfiltered_detections\"\n  input_side_packet: \"ANCHORS:facedetectionshortrange__facedetection__anchors\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.TensorsToDetectionsCalculatorOptions\"\n    value: \"\\010\\001\\030\\020H\\004P\\006X\\002`\\000p\\001x\\001\\205\\001\\000\\000\\310B\\020\\200\\007%\\000\\000\\000C-\\000\\000\\000C=\\000\\000\\000C5\\000\\000\\000C\\235\\001\\000\\000\\000?\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__NonMaxSuppressionCalculator\"\n  calculator: \"NonMaxSuppressionCalculator\"\n  input_stream: \"facedetectionshortrange__facedetection__unfiltered_detections\"\n  output_stream: \"facedetectionshortrange__facedetection__filtered_detections\"\n  options {\n    [mediapipe.NonMaxSuppressionCalculatorOptions.ext] {\n      min_suppression_threshold: 0.3\n      overlap_type: INTERSECTION_OVER_UNION\n      algorithm: WEIGHTED\n    }\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__DetectionProjectionCalculator\"\n  calculator: \"DetectionProjectionCalculator\"\n  input_stream: \"DETECTIONS:facedetectionshortrange__facedetection__filtered_detections\"\n  input_stream: \"PROJECTION_MATRIX:facedetectionshortrange__facedetection__transform_matrix\"\n  output_stream: \"DETECTIONS:detections\"\n}\ninput_stream: \"IMAGE:image\"\nexecutor {\n}\noutput_stream: \"DETECTIONS:detections\"\ntype: \"FaceDetectionShortRangeCpu\"\ngraph_options {\n  type_url: \"type.googleapis.com/mediapipe.FaceDetectionOptions\"\n  value: \"\\245\\002\\000\\000\\000?\"\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mface_encoder\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FaceEncoder, store_encodings\n\u001B[0;32m----> 3\u001B[0m face_encoder \u001B[38;5;241m=\u001B[39m \u001B[43mFaceEncoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m image_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../artefacts/incoming/kiki2.jpg\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/Perso/IA/face_detection/src/face_encoder.py:44\u001B[0m, in \u001B[0;36mFaceEncoder.__init__\u001B[0;34m(self, face_rec_model_path, predictor_path)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mface_rec_model \u001B[38;5;241m=\u001B[39m dlib\u001B[38;5;241m.\u001B[39mface_recognition_model_v1(face_rec_model_path)\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredictor \u001B[38;5;241m=\u001B[39m dlib\u001B[38;5;241m.\u001B[39mshape_predictor(predictor_path)\n\u001B[0;32m---> 44\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmp_face_detection \u001B[38;5;241m=\u001B[39m \u001B[43mmp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msolutions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mface_detection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFaceDetection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmin_detection_confidence\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Perso/IA/face_detection/venv/lib/python3.12/site-packages/mediapipe/python/solutions/face_detection.py:82\u001B[0m, in \u001B[0;36mFaceDetection.__init__\u001B[0;34m(self, min_detection_confidence, model_selection)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Initializes a MediaPipe Face Detection object.\u001B[39;00m\n\u001B[1;32m     69\u001B[0m \n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;124;03m    https://solutions.mediapipe.dev/face_detection#model_selection.\u001B[39;00m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     80\u001B[0m binary_graph_path \u001B[38;5;241m=\u001B[39m _FULL_RANGE_GRAPH_FILE_PATH \u001B[38;5;28;01mif\u001B[39;00m model_selection \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m _SHORT_RANGE_GRAPH_FILE_PATH\n\u001B[0;32m---> 82\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     83\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbinary_graph_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbinary_graph_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     84\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgraph_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_graph_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[43m        \u001B[49m\u001B[43mface_detection_pb2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFaceDetectionOptions\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmin_score_thresh\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_detection_confidence\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdetections\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Perso/IA/face_detection/venv/lib/python3.12/site-packages/mediapipe/python/solution_base.py:248\u001B[0m, in \u001B[0;36mSolutionBase.__init__\u001B[0;34m(self, binary_graph_path, graph_config, calculator_params, graph_options, side_inputs, outputs, stream_type_hints)\u001B[0m\n\u001B[1;32m    244\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m graph_options:\n\u001B[1;32m    245\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_extension(canonical_graph_config_proto\u001B[38;5;241m.\u001B[39mgraph_options,\n\u001B[1;32m    246\u001B[0m                       graph_options)\n\u001B[0;32m--> 248\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph \u001B[38;5;241m=\u001B[39m \u001B[43mcalculator_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCalculatorGraph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgraph_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcanonical_graph_config_proto\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_simulated_timestamp \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph_outputs \u001B[38;5;241m=\u001B[39m {}\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Failed to parse: node {\n  name: \"facedetectionshortrange__facedetection__ToImageCalculator\"\n  calculator: \"ToImageCalculator\"\n  input_stream: \"IMAGE:image\"\n  output_stream: \"IMAGE:facedetectionshortrange__facedetection__multi_backend_image\"\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__ImageToTensorCalculator\"\n  calculator: \"ImageToTensorCalculator\"\n  input_stream: \"IMAGE:facedetectionshortrange__facedetection__multi_backend_image\"\n  output_stream: \"TENSORS:facedetectionshortrange__facedetection__input_tensors\"\n  output_stream: \"MATRIX:facedetectionshortrange__facedetection__transform_matrix\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.ImageToTensorCalculatorOptions\"\n    value: \"\\030\\001\"\\n\\r\\000\\000\\200\\277\\025\\000\\000\\200?0\\001\\010\\200\\001\\020\\200\\001\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__SsdAnchorsCalculator\"\n  calculator: \"SsdAnchorsCalculator\"\n  output_side_packet: \"facedetectionshortrange__facedetection__anchors\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.SsdAnchorsCalculatorOptions\"\n    value: \"\\035\\000\\000\\030>%\\000\\000@?-\\000\\000\\000?5\\000\\000\\000?]\\000\\000\\200?p\\001\\010\\200\\001\\020\\200\\0018\\004P\\010P\\020P\\020P\\020m\\000\\000\\200?\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__inferencecalculator__facedetectionshortrange__facedetection__InferenceCalculator\"\n  calculator: \"InferenceCalculatorCpu\"\n  input_stream: \"TENSORS:facedetectionshortrange__facedetection__input_tensors\"\n  output_stream: \"TENSORS:facedetectionshortrange__facedetection__detection_tensors\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.InferenceCalculatorOptions\"\n    value: \"*\\002\"\\000\\nBmediapipe/modules/face_detection/face_detection_short_range.tflite\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__TensorsToDetectionsCalculator\"\n  calculator: \"TensorsToDetectionsCalculator\"\n  input_stream: \"TENSORS:facedetectionshortrange__facedetection__detection_tensors\"\n  output_stream: \"DETECTIONS:facedetectionshortrange__facedetection__unfiltered_detections\"\n  input_side_packet: \"ANCHORS:facedetectionshortrange__facedetection__anchors\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.TensorsToDetectionsCalculatorOptions\"\n    value: \"\\010\\001\\030\\020H\\004P\\006X\\002`\\000p\\001x\\001\\205\\001\\000\\000\\310B\\020\\200\\007%\\000\\000\\000C-\\000\\000\\000C=\\000\\000\\000C5\\000\\000\\000C\\235\\001\\000\\000\\000?\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__NonMaxSuppressionCalculator\"\n  calculator: \"NonMaxSuppressionCalculator\"\n  input_stream: \"facedetectionshortrange__facedetection__unfiltered_detections\"\n  output_stream: \"facedetectionshortrange__facedetection__filtered_detections\"\n  options {\n    [mediapipe.NonMaxSuppressionCalculatorOptions.ext] {\n      min_suppression_threshold: 0.3\n      overlap_type: INTERSECTION_OVER_UNION\n      algorithm: WEIGHTED\n    }\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__DetectionProjectionCalculator\"\n  calculator: \"DetectionProjectionCalculator\"\n  input_stream: \"DETECTIONS:facedetectionshortrange__facedetection__filtered_detections\"\n  input_stream: \"PROJECTION_MATRIX:facedetectionshortrange__facedetection__transform_matrix\"\n  output_stream: \"DETECTIONS:detections\"\n}\ninput_stream: \"IMAGE:image\"\nexecutor {\n}\noutput_stream: \"DETECTIONS:detections\"\ntype: \"FaceDetectionShortRangeCpu\"\ngraph_options {\n  type_url: \"type.googleapis.com/mediapipe.FaceDetectionOptions\"\n  value: \"\\245\\002\\000\\000\\000?\"\n}\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
